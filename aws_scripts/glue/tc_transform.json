{
	"jobConfig": {
		"name": "tc_transform",
		"description": "",
		"role": "arn:aws:iam::554575472400:role/service-role/AWSGlueServiceRole",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "tc_transform.py",
		"scriptLocation": "s3://aws-glue-assets-554575472400-us-east-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-09-16T00:49:33.894Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-554575472400-us-east-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-554575472400-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql.functions import col, sum as _sum, avg as _avg, count as _count\r\n\r\n# ---------------------------\r\n# Configuração básica do Glue\r\n# ---------------------------\r\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args[\"JOB_NAME\"], args)\r\n\r\n# ---------------------------\r\n# Leitura dos dados D0 e D-2\r\n# ---------------------------\r\npath_d0 = \"s3://bucket-refined-data-tech2/data_execucao_d0/\"\r\npath_d2 = \"s3://bucket-refined-data-tech2/data_execucao_d2/\"\r\n\r\ndf_d0 = spark.read.parquet(path_d0)\r\ndf_d2 = spark.read.parquet(path_d2)\r\n\r\nprint(\"Schema D0:\")\r\ndf_d0.printSchema()\r\nprint(\"Schema D-2:\")\r\ndf_d2.printSchema()\r\n\r\n# ---------------------------\r\n# Requisito 5.A: Agrupamento em D0\r\n# ---------------------------\r\ndf_agg_d0 = (\r\n    df_d0.groupBy(\"codigo\", \"acao\", \"dt_ptcm\")\r\n    .agg(\r\n        _sum(\"qt_teorica\").alias(\"soma_qt_teorica\"),\r\n        _avg(\"pc_participacao\").alias(\"media_pc_participacao\"),\r\n        _count(\"*\").alias(\"qtd_registros\")\r\n    )\r\n)\r\n\r\nprint(\"Resultado do agrupamento (5.A):\")\r\ndf_agg_d0.show(5, truncate=False)\r\n\r\n# ---------------------------\r\n# Requisito 5.C: Comparação entre D0 e D-2\r\n# ---------------------------\r\ndf_comparacao = (\r\n    df_d0.alias(\"d0\")\r\n    .join(\r\n        df_d2.alias(\"d2\"),\r\n        on=[\"codigo\", \"acao\"],  # join por código e nome da ação\r\n        how=\"left\"              # mantém os de D0, mesmo que não existam em D-2\r\n    )\r\n    .select(\r\n        col(\"codigo\"),\r\n        col(\"acao\"),\r\n        col(\"d0.dt_ptcm\"),\r\n        (col(\"d0.qt_teorica\") - col(\"d2.qt_teorica\")).alias(\"diferenca_qt_teorica\"),\r\n        (col(\"d0.pc_participacao\") - col(\"d2.pc_participacao\")).alias(\"diferenca_pc_participacao\")\r\n    )\r\n)\r\n\r\nprint(\"Comparação D0 x D-2 (5.C):\")\r\ndf_comparacao.show(5, truncate=False)\r\n\r\n# ---------------------------\r\n# Salvar resultados no S3\r\n# ---------------------------\r\noutput_base = \"s3://bucket-refined-data-tech2/output_tc_transform/\"\r\n\r\ndf_agg_d0.write.mode(\"overwrite\").parquet(output_base + \"agrupamento_d0/\")\r\ndf_comparacao.write.mode(\"overwrite\").parquet(output_base + \"comparacao_d0_d2/\")\r\n\r\n# ---------------------------\r\n# Finalização do job\r\n# ---------------------------\r\njob.commit()\r\n"
}