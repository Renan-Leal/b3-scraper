{
	"jobConfig": {
		"name": "tc_load",
		"description": "",
		"role": "arn:aws:iam::554575472400:role/service-role/AWSGlueServiceRole",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "tc_load.py",
		"scriptLocation": "s3://aws-glue-assets-554575472400-us-east-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-09-16T23:24:53.775Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-554575472400-us-east-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-554575472400-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nimport boto3\r\nimport logging\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql.functions import col, sum as _sum, avg as _avg, count as _count\r\n\r\n# ---------------------------\r\n# Configuração inicial\r\n# ---------------------------\r\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args[\"JOB_NAME\"], args)\r\n\r\n# Evita que as partições sejam apagadas\r\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n\r\nlogger = logging.getLogger()\r\nlogger.setLevel(logging.INFO)\r\n\r\n# ---------------------------\r\n# Caminhos de entrada e saída\r\n# ---------------------------\r\ninput_d0 = \"s3://bucket-refined-data-tech2/output_tc_transform/agrupamento_d0/\"\r\ninput_d2 = \"s3://bucket-refined-data-tech2/output_tc_transform/comparacao_d0_d2/\"\r\noutput_final = \"s3://bucket-refined-data-tech2/tables/\"\r\n\r\n# ---------------------------\r\n# Leitura dos dados\r\n# ---------------------------\r\ndf_d0 = spark.read.parquet(input_d0)\r\ndf_d2 = spark.read.parquet(input_d2)\r\n\r\n# ---------------------------\r\n# Escrita dos dados no S3\r\n# ---------------------------\r\n\r\n# tb_agrupamento -> com partição\r\n(\r\n    df_d0.write\r\n    .mode(\"overwrite\")\r\n    .partitionBy(\"dt_ptcm\", \"acao\")\r\n    .parquet(output_final + \"agrupamento/\")\r\n)\r\n\r\n# tb_comparacao -> sem partição\r\n(\r\n    df_d2.write\r\n    .mode(\"overwrite\")\r\n    .parquet(output_final + \"comparacao/\")\r\n)\r\n\r\n# ---------------------------\r\n# Glue Catalog - Criação/Atualização\r\n# ---------------------------\r\ndatabase_name = \"db_tech2_refined\"\r\nglue_client = boto3.client(\"glue\")\r\n\r\n# Garantir que DB existe\r\ntry:\r\n    glue_client.get_database(Name=database_name)\r\nexcept glue_client.exceptions.EntityNotFoundException:\r\n    glue_client.create_database(DatabaseInput={\"Name\": database_name})\r\n    logger.info(f\"Database '{database_name}' criado.\")\r\n\r\n# ---------------------------\r\n# Definição das tabelas\r\n# ---------------------------\r\ntabelas = [\r\n    {\r\n        \"nome\": \"tb_agrupamento\",\r\n        \"path\": output_final + \"agrupamento/\",\r\n        \"colunas\": [\r\n            {\"Name\": \"codigo\", \"Type\": \"string\"},\r\n            {\"Name\": \"soma_qt_teorica\", \"Type\": \"bigint\"},\r\n            {\"Name\": \"media_pc_participacao\", \"Type\": \"double\"},\r\n            {\"Name\": \"qtd_registros\", \"Type\": \"bigint\"},\r\n        ],\r\n        \"particoes\": [\r\n            {\"Name\": \"dt_ptcm\", \"Type\": \"string\"},\r\n            {\"Name\": \"acao\", \"Type\": \"string\"},\r\n        ]\r\n    },\r\n    {\r\n        \"nome\": \"tb_comparacao\",\r\n        \"path\": output_final + \"comparacao/\",\r\n        \"colunas\": [\r\n            {\"Name\": \"codigo\", \"Type\": \"string\"},\r\n            {\"Name\": \"acao\", \"Type\": \"string\"},\r\n            {\"Name\": \"data_atual\", \"Type\": \"string\"},\r\n            {\"Name\": \"diff_qt_teorica\", \"Type\": \"bigint\"},\r\n            {\"Name\": \"diff_pc_participacao\", \"Type\": \"double\"},\r\n        ],\r\n        \"particoes\": []  # sem partição\r\n    }\r\n]\r\n\r\nfor tbl in tabelas:\r\n    table_input = {\r\n        \"Name\": tbl[\"nome\"],\r\n        \"StorageDescriptor\": {\r\n            \"Columns\": tbl[\"colunas\"],\r\n            \"Location\": tbl[\"path\"],\r\n            \"InputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\r\n            \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\r\n            \"SerdeInfo\": {\r\n                \"SerializationLibrary\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\r\n                \"Parameters\": {\"serialization.format\": \"1\"}\r\n            }\r\n        },\r\n        \"PartitionKeys\": tbl[\"particoes\"],\r\n        \"TableType\": \"EXTERNAL_TABLE\"\r\n    }\r\n\r\n    try:\r\n        glue_client.get_table(DatabaseName=database_name, Name=tbl[\"nome\"])\r\n        glue_client.update_table(DatabaseName=database_name, TableInput=table_input)\r\n        logger.info(f\"Tabela '{tbl['nome']}' atualizada no Glue Catalog.\")\r\n    except glue_client.exceptions.EntityNotFoundException:\r\n        glue_client.create_table(DatabaseName=database_name, TableInput=table_input)\r\n        logger.info(f\"Tabela '{tbl['nome']}' criada no Glue Catalog.\")\r\n\r\n# ---------------------------\r\n# Reparar partições apenas nas tabelas particionadas\r\n# ---------------------------\r\nfor tbl in tabelas:\r\n    if tbl[\"particoes\"]:\r\n        repair_sql = f\"MSCK REPAIR TABLE {database_name}.{tbl['nome']}\"\r\n        logger.info(f\"Executando: {repair_sql}\")\r\n        spark.sql(repair_sql)\r\n\r\njob.commit()\r\n"
}