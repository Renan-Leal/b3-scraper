{
	"jobConfig": {
		"name": "tc_extract",
		"description": "",
		"role": "arn:aws:iam::554575472400:role/service-role/AWSGlueServiceRole",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "tc_extract.py",
		"scriptLocation": "s3://aws-glue-assets-554575472400-us-east-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-09-16T00:48:55.723Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-554575472400-us-east-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-554575472400-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql import DataFrame\r\nfrom pyspark.sql.functions import col, regexp_replace, sum as _sum, count as _count, avg as _avg, to_date, datediff, lit\r\nfrom pyspark.sql.types import StructType, StructField, StringType\r\n\r\n# Argumentos vindos da Lambda\r\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"DATA_EXECUCAO\", \"DATA_EXECUCAO_D2\"])\r\n\r\n# Criação dos contextos\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args[\"JOB_NAME\"], args)\r\n\r\n# Partições recebidas\r\ndata_execucao = args[\"DATA_EXECUCAO\"]\r\ndata_execucao_d2 = args[\"DATA_EXECUCAO_D2\"]\r\n\r\nprint(f\"Executando para D0: {data_execucao}, D-2: {data_execucao_d2 if data_execucao_d2 else 'não enviado'}\")\r\n\r\n# Caminho no S3 para D0\r\npath_d0 = f\"s3://bucket-raw-data-tech2/dados_b3/dt_ptcm={data_execucao}/dados_b3_{data_execucao}.parquet\"\r\n\r\n# ---------------------------\r\n# Função utilitária para renomear colunas\r\n# ---------------------------\r\ndef rename_columns(df: DataFrame, rename_map: dict) -> DataFrame:\r\n    for old_col, new_col in rename_map.items():\r\n        if old_col in df.columns:\r\n            df = df.withColumnRenamed(old_col, new_col)\r\n    return df\r\n\r\n# ---------------------------\r\n# Leitura da partição D0\r\n# ---------------------------\r\ndf_d0 = spark.read.parquet(path_d0)\r\n\r\n# Leitura da partição D-2 (se existir)\r\nif data_execucao_d2:\r\n    path_d2 = f\"s3://bucket-raw-data-tech2/dados_b3/dt_ptcm={data_execucao_d2}/dados_b3_{data_execucao_d2}.parquet\"\r\n    try:\r\n        df_d2 = spark.read.parquet(path_d2)\r\n    except Exception as e:\r\n        print(f\"Aviso: não foi possível ler D-2 ({data_execucao_d2}). Erro: {e}\")\r\n        df_d2 = spark.createDataFrame([], schema=df_d0.schema)  # cria DF vazio com mesmo schema\r\nelse:\r\n    print(\"Nenhuma data D-2 enviada. Criando DataFrame vazio.\")\r\n    df_d2 = spark.createDataFrame([], schema=df_d0.schema)\r\n\r\n# ---------------------------\r\n# Requisito 5.B: renomeando colunas\r\n# ---------------------------\r\nrename_map = {\r\n    \"Código\": \"codigo\",\r\n    \"Ação\": \"acao\",\r\n    \"Tipo\": \"tipo\",\r\n    \"Qtde. Teórica\": \"qt_teorica\",\r\n    \"Part. (%)\": \"pc_participacao\"\r\n}\r\n\r\ndf_d0 = rename_columns(df_d0, rename_map)\r\ndf_d2 = rename_columns(df_d2, rename_map)\r\n\r\n# Normalizando colunas numéricas D0\r\ndf_d0 = df_d0.withColumn(\"qt_teorica\", regexp_replace(col(\"qt_teorica\"), r\"\\.\", \"\").cast(\"long\"))\r\ndf_d0 = df_d0.withColumn(\"pc_participacao\", regexp_replace(col(\"pc_participacao\"), \",\", \".\").cast(\"double\"))\r\n\r\n# Normalizando colunas numéricas D-2\r\nif df_d2.columns:  # só aplica se não for vazio\r\n    df_d2 = df_d2.withColumn(\"qt_teorica\", regexp_replace(col(\"qt_teorica\"), r\"\\.\", \"\").cast(\"long\"))\r\n    df_d2 = df_d2.withColumn(\"pc_participacao\", regexp_replace(col(\"pc_participacao\"), \",\", \".\").cast(\"double\"))\r\n\r\nprint(\"Esquema D0: \")\r\ndf_d0.printSchema()\r\ndf_d0.show(5)\r\n\r\nprint(\"Esquema D-2:\")\r\ndf_d2.printSchema()\r\ndf_d2.show(5)\r\n\r\n# diretório S3 para salvar os dados brutos particionados\r\nraw_data_path_d0 = \"s3://bucket-refined-data-tech2/data_execucao_d0\"\r\nraw_data_path_d2 = \"s3://bucket-refined-data-tech2/data_execucao_d2\"\r\n\r\n# salva os dados brutos em formato parquet no S3\r\ndf_d0.write.mode(\"overwrite\").parquet(raw_data_path_d0)\r\ndf_d2.write.mode(\"overwrite\").parquet(raw_data_path_d2)\r\n"
}